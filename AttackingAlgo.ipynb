{"cells":[{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["\"\"\"\n","Notation:\n","  the class of node k: c(k)\n","  the class i: c_i\n","  all neighbors of k: N(k)\n","  all neighbors of k in the class: N(k)=c_i\n","\n","build a model:\n","\tInput: features of N(k)=c_i   ,  for any i\n","  Try to predict: c(k)\n","\n","if use GCN, loss = preds - c(k); if use logistic regression, calc probability. But 作用相同，以下用loss举例\n","\n","loss results example:\n","\t一个node(eg: A)有10个neighbors，这些neighbors有3类class label，用这三类分别去predict A的label,\n","  \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t得到有loss_1, loss_2, loss_3，loss_1最小，需要算下avg(loss_2+loss_3)\n","\t如果avg(loss_2+loss_3)是top 50大的，这个node是我们要攻击的\n","  如果被攻击，因为loss_1最小, 所以class为1的neighbors是我们要抢的\n","\n","intuition:\n","\n","选择抢的neighbors:\n","  the smaller loss means this class of neighbors provide more info to predict c(k)\n","  既然作用大，那就抢它\n","  then, we changes edges (this class of neighbors, k) to (this class of neighbors, fake node of k)\n","  注意cora的edge是要注意方向的\n","\n","选择attack的node:\n","  neighbors里面已经抢掉了最小loss的class，剩下的class的loss的average越大，代表剩下的这些neighbors对于确定c(k)作用越小，\n","  那么我们就攻击这个\n","   -> 再想一下这个思路，剩下的loss的avg不一定最好，万一原本就很差呢？或许看最小loss有多小会更好（i.e.抢完neighbors损失的信息更多）？\n","   \t\t或者看avg(loss_1+loss_2+loss_3)-avg(loss_2+loss_3)会更好？ 可以分别试一下看看结果！\n","  创建一个fake node with different class label （怎么去选这个label？随机？）\n","\"\"\""]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-12-12T06:38:23.951779Z","iopub.status.busy":"2022-12-12T06:38:23.951266Z","iopub.status.idle":"2022-12-12T06:39:00.270622Z","shell.execute_reply":"2022-12-12T06:39:00.269155Z","shell.execute_reply.started":"2022-12-12T06:38:23.951680Z"},"trusted":true},"outputs":[],"source":["!pip install stellargraph"]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2022-12-12T06:39:00.275410Z","iopub.status.busy":"2022-12-12T06:39:00.274263Z","iopub.status.idle":"2022-12-12T06:39:08.886640Z","shell.execute_reply":"2022-12-12T06:39:08.885041Z","shell.execute_reply.started":"2022-12-12T06:39:00.275364Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["2022-12-12 06:39:06.975401: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.\n"]}],"source":["import stellargraph as sg\n","from stellargraph.mapper import FullBatchNodeGenerator\n","from stellargraph.layer import GCN\n","\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","import matplotlib.pyplot as plt\n","from tqdm import tqdm, trange\n","\n","import numpy as np\n","import tensorflow as tf\n","from tensorflow import keras\n","from tensorflow.keras import layers\n","import matplotlib.pyplot as plt\n","import pandas as pd\n","import json\n","import os\n","import warnings\n","from copy import deepcopy\n","from tqdm import tqdm\n","import tensorflow_addons as tfa\n","warnings.filterwarnings(\"ignore\")"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2022-12-12T06:39:08.889036Z","iopub.status.busy":"2022-12-12T06:39:08.888363Z","iopub.status.idle":"2022-12-12T06:39:08.899508Z","shell.execute_reply":"2022-12-12T06:39:08.898515Z","shell.execute_reply.started":"2022-12-12T06:39:08.888997Z"},"trusted":true},"outputs":[],"source":["class NeighborModel(nn.Module):\n","    def __init__(self, num_features, num_classes):\n","        super().__init__()\n","        self.ff1 = nn.Sequential(\n","            nn.BatchNorm1d(num_features),\n","            nn.Dropout(0.5),\n","            nn.Linear(num_features, 32),\n","            nn.GELU()\n","        )\n","        self.ff2 = nn.Sequential(\n","            nn.BatchNorm1d(32),\n","            nn.Dropout(0.5),\n","            nn.Linear(32, 32),\n","            nn.GELU()\n","        )\n","        self.ff3 = nn.Sequential(\n","            nn.BatchNorm1d(32),\n","            nn.Dropout(0.5),\n","            nn.Linear(32, 32),\n","            nn.GELU()\n","        )\n","        self.top = nn.Linear(32, num_classes)\n","        \n","\n","    def forward(self, x):\n","        x = self.ff1(x)\n","        x = F.normalize(x)\n","        x = self.ff2(x) + x\n","        x = self.ff3(x) + x\n","        x = self.top(x)\n","        return x"]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2022-12-12T06:39:08.902834Z","iopub.status.busy":"2022-12-12T06:39:08.901802Z","iopub.status.idle":"2022-12-12T06:39:08.933567Z","shell.execute_reply":"2022-12-12T06:39:08.932442Z","shell.execute_reply.started":"2022-12-12T06:39:08.902781Z"},"trusted":true},"outputs":[],"source":["def test(G, node_subjects, classifier_name, attack_num=0):\n","    train_subjects, val_subjects = model_selection.train_test_split(\n","        node_subjects, train_size=0.8, test_size=None, stratify=node_subjects,\n","        random_state=12345, shuffle=True\n","    )\n","    test_subjects = node_subjects[:-attack_num] if attack_num else node_subjects\n","    target_encoding = preprocessing.LabelBinarizer()\n","\n","    train_targets = target_encoding.fit_transform(train_subjects)\n","    val_targets = target_encoding.transform(val_subjects)\n","    test_targets = target_encoding.transform(test_subjects)\n","\n","\n","    def setup_SGC():\n","        generator = FullBatchNodeGenerator(G, method=\"sgc\", k=2)\n","        classifier_model = GCN(\n","            layer_sizes=[train_targets.shape[1]],\n","            generator=generator,\n","            bias=True,\n","            dropout=0.5,\n","            activations=[\"softmax\"],\n","            kernel_regularizer=regularizers.l2(5e-4),\n","        )\n","        return generator, classifier_model\n","\n","\n","    def setup_GCN():\n","        generator = FullBatchNodeGenerator(G, method=\"gcn\")\n","        classifier_model = GCN(\n","            layer_sizes=[32, 32], activations=[\"relu\", \"relu\"], generator=generator, dropout=0.5\n","        )\n","        return generator, classifier_model\n","\n","\n","    def setup_GraphSAGE():\n","        generator = GraphSAGENodeGenerator(G, 50, [10,5])\n","        classifier_model = GraphSAGE(\n","            layer_sizes=[32, 32], generator=generator, bias=False, dropout=0.5,\n","        )\n","        return generator, classifier_model\n","\n","    def setup_GAT():\n","        generator = FullBatchNodeGenerator(G, method=\"gat\")\n","        classifier_model = GAT(\n","            layer_sizes=[8, train_targets.shape[1]],\n","            activations=[\"elu\", \"softmax\"],\n","            attn_heads=8,\n","            generator=generator,\n","            in_dropout=0.5,\n","            attn_dropout=0.5,\n","            normalize=None,\n","        )\n","        return generator, classifier_model\n","\n","    if classifier_name == \"GCN\":\n","        generator, classifier_model = setup_GCN()\n","        es_callback = EarlyStopping(monitor=\"val_acc\", patience=50, restore_best_weights=True)\n","    elif classifier_name == \"GraphSAGE\":\n","        generator, classifier_model = setup_GraphSAGE()\n","    elif classifier_name == \"GAT\":\n","        generator, classifier_model = setup_GAT()\n","        if not os.path.isdir(\"logs\"):\n","            os.makedirs(\"logs\")\n","        es_callback = EarlyStopping(\n","            monitor=\"val_acc\", patience=50\n","        )  # patience is the number of epochs to wait before early stopping in case of no further improvement\n","        mc_callback = ModelCheckpoint(\n","            \"logs/best_model.h5\", monitor=\"val_acc\", save_best_only=True, save_weights_only=True\n","        )\n","    elif classifier_name == \"SGC\":\n","        generator, classifier_model = setup_SGC()\n","        if not os.path.isdir(\"logs\"):\n","            os.makedirs(\"logs\")\n","        es_callback = EarlyStopping(\n","            monitor=\"val_acc\", patience=50\n","        )  # patience is the number of epochs to wait before early stopping in case of no further improvement\n","        mc_callback = ModelCheckpoint(\n","            \"logs/best_model.h5\", monitor=\"val_acc\", save_best_only=True, save_weights_only=True\n","        )\n","\n","\n","\n","    train_gen = generator.flow(train_subjects.index, train_targets)\n","\n","    x_inp, x_out = classifier_model.in_out_tensors()\n","\n","    predictions = layers.Dense(units=train_targets.shape[1], activation=\"softmax\")(x_out)\n","\n","    val_gen = generator.flow(val_subjects.index, val_targets)\n","\n","    test_gen = generator.flow(test_subjects.index, test_targets)\n","\n","    tqdm_callback = tfa.callbacks.TQDMProgressBar(show_epoch_progress=False)\n","\n","    if classifier_name == \"GAT\":\n","        model = Model(inputs=x_inp, outputs=x_out)\n","        model.compile(\n","            optimizer=optimizers.Adam(lr=0.005),\n","            loss=losses.categorical_crossentropy,\n","            metrics=[\"acc\"],\n","        )\n","        history = model.fit(\n","            train_gen,\n","            epochs=100,\n","            validation_data=val_gen,\n","            verbose=0,\n","            shuffle=False,  # this should be False, since shuffling data means shuffling the whole graph\n","            callbacks=[es_callback, mc_callback, tqdm_callback],\n","        )\n","    elif classifier_name == \"GCN\":\n","        model = Model(inputs=x_inp, outputs=predictions)\n","        model.compile(\n","            optimizer=optimizers.Adam(lr=0.01),\n","            loss=losses.categorical_crossentropy,\n","            metrics=[\"acc\"],\n","        )\n","        history = model.fit(\n","            train_gen,\n","            epochs=500,\n","            validation_data=val_gen,\n","            verbose=0,\n","            shuffle=False,  # this should be False, since shuffling data means shuffling the whole graph\n","            callbacks=[es_callback, tqdm_callback],\n","        )\n","    elif classifier_name == \"GraphSAGE\":\n","        model = Model(inputs=x_inp, outputs=predictions)\n","        model.compile(\n","            optimizer=optimizers.Adam(lr=0.2),\n","            loss=losses.categorical_crossentropy,\n","            metrics=[\"acc\"],\n","        )\n","        history = model.fit(\n","            train_gen, epochs=20, validation_data=test_gen, verbose=2, shuffle=False\n","        )\n","    elif classifier_name == \"SGC\":\n","        model = Model(inputs=x_inp, outputs=x_out)\n","        model.compile(\n","            optimizer=optimizers.Adam(lr=0.2),\n","            loss=losses.categorical_crossentropy,\n","            metrics=[\"acc\"],\n","        )\n","        history = model.fit(\n","            train_gen,\n","            epochs=200,\n","            validation_data=val_gen,\n","            verbose=0,\n","            shuffle=False,  # this should be False, since shuffling data means shuffling the whole graph\n","            callbacks=[es_callback, mc_callback, tqdm_callback],\n","        )\n","\n","\n","#     sg.utils.plot_history(history)\n","\n","    test_metrics = model.evaluate(test_gen)\n","#     print(\"\\nTest Set Metrics:\")\n","#     for name, val in zip(model.metrics_names, test_metrics):\n","#         print(\"\\t{}: {:0.4f}\".format(name, val))\n","    del model\n","    return test_metrics[1]"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2022-12-12T06:39:08.935899Z","iopub.status.busy":"2022-12-12T06:39:08.935260Z","iopub.status.idle":"2022-12-12T06:39:08.973977Z","shell.execute_reply":"2022-12-12T06:39:08.972712Z","shell.execute_reply.started":"2022-12-12T06:39:08.935865Z"},"trusted":true},"outputs":[],"source":["class AttackingAlgo():\n","    def __init__(self, node_ids, features, edges, targets):\n","        paper_idx = {name: idx for idx, name in enumerate(node_ids)}\n","        self.node_ids = [paper_idx[i] for i in node_ids]\n","        self.edges = [[paper_idx[n1], paper_idx[n2]] for n1,n2 in edges]\n","        \n","        self.features = features\n","        \n","        class_values = sorted(targets.unique())\n","        class_idx = {name: id for id, name in enumerate(class_values)}\n","        targets = targets.apply(lambda value: class_idx[value])\n","        self.targets = targets.to_numpy()\n","        \n","        self.num_classes = len(class_values)\n","        \n","        self.find_neighbors()\n","    \n","    def calc_loss(self, neighbors_features, node_targets):\n","        neighbors_features = torch.from_numpy(neighbors_features).float()\n","        targets = torch.from_numpy(node_targets).float().type(torch.LongTensor)\n","        \n","        device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n","\n","        model = NeighborModel(neighbors_features.shape[1], self.num_classes)\n","        model.to(device)\n","\n","        criterion = nn.CrossEntropyLoss().cuda() if torch.cuda.is_available() else nn.CrossEntropyLoss()\n","        optimizer = optim.Adam(model.parameters(), lr=1e-2)\n","\n","        for epoch in range(1, 101):\n","            running_loss = 0.0\n","            for i in range(0, len(neighbors_features), 256):\n","                # get the inputs; data is a list of [inputs, labels]\n","                inputs, labels = neighbors_features[i:i+256].to(device), targets[i:i+256].to(device)\n","\n","                # zero the parameter gradients\n","                optimizer.zero_grad()\n","\n","                # forward + backward + optimize\n","                outputs = model(inputs)\n","                loss = criterion(outputs, labels)\n","                loss.backward()\n","                optimizer.step()\n","\n","                running_loss += loss.item()\n","        \n","        node_targets = F.one_hot(torch.from_numpy(node_targets))\n","        \n","        loss = []\n","        with torch.no_grad():\n","            for i in range(0, len(neighbors_features), 256):\n","                # get the inputs; data is a list of [inputs, labels]\n","                inputs = neighbors_features[i:i+256].to(device)\n","                # calculate outputs by running images through the network\n","                outputs = model(inputs)\n","                loss.extend(-np.sum(node_targets[i:i+256].numpy() * np.ma.log(outputs.cpu().numpy()).filled(0), axis=1))\n","        \n","        loss_mat = np.zeros((len(self.node_ids), self.num_classes))\n","        index = 0\n","        for i in range(len(self.node_ids)):\n","            for j in range(self.num_classes):\n","                if (i, j) in self.pos:\n","                    loss_mat[i, j] = loss[index]\n","                    index += 1\n","                else:\n","                    loss_mat[i, j] = None\n","        return loss_mat\n","    \n","    def generate_inputs(self):\n","        self.pos = []\n","        neighbors_features = []\n","        node_targets = []\n","        for i in range(self.num_classes):\n","            for j in self.node_ids:\n","                temp = []\n","                for n in self.neighbors[j]:\n","                    if self.targets[n] == i:\n","                        temp.append(self.features[n])\n","                if len(temp) != 0:\n","                    self.pos.append((j, i))\n","                    node_targets.append(self.targets[j])\n","                    neighbors_features.append(np.concatenate((self.features[j], np.mean(temp, axis=0))))\n","            \n","        return np.array(neighbors_features), np.array(node_targets)\n","    \n","    def find_neighbors(self):\n","        self.neighbors = {i:[] for i in self.node_ids}\n","        for n1, n2 in self.edges:\n","            self.neighbors[n1].append(n2)\n","            \n","    def test(self):\n","        neighbors_features, node_targets = self.generate_inputs()\n","        return self.calc_loss(neighbors_features, node_targets)\n","    \n","    def attack(self, attack_num=50, loss_function=2):\n","        # loss function\n","        # 1. AVE without lowest loss\n","        # 2. Minimum loss\n","        # 3. ave(all loss) - ave(loss without lowest)\n","        \n","        neighbors_features, node_targets = self.generate_inputs()\n","        loss_mat = self.calc_loss(neighbors_features, node_targets)\n","        # temp loss mat for coding\n","        k = len(self.node_ids)\n","        \n","        \n","        node_loss = []\n","        node_neigh_class = [] # the class with min_loss\n","        node_heighest_loss_class = []\n","        for each_node in loss_mat:\n","    \n","            valid_loss = each_node[~np.isnan(each_node)]\n","            if len(valid_loss) < 2:\n","                node_loss.append(10**10)\n","                node_neigh_class.append(-1)\n","                node_heighest_loss_class.append(-1)\n","                continue\n","            min_loss = min(valid_loss)\n","            max_loss = max(valid_loss)\n","            min_class = np.where(valid_loss==min_loss)[0][0]\n","            max_class = np.where(valid_loss==max_loss)[0][0]\n","            ave_loss_without_min = (sum(valid_loss)-min_loss)/len(valid_loss-1)\n","            node_heighest_loss_class.append(max_class)\n","            \n","            # 1. AVE without lowest loss\n","            if loss_function ==1:\n","                node_loss.append(ave_loss_without_min)\n","            \n","            # 2. Minimum loss\n","            elif loss_function == 2:\n","                node_loss.append(min_loss)\n","            \n","            # 3. ave(all loss) - ave( loss without lowest)\n","            else:\n","                ave_diff = sum(valid_loss)/len(valid_loss) - ave_loss_without_min\n","                node_loss.append(ave_diff)\n","             \n","            node_neigh_class.append(min_class)\n","\n","\n","        node_loss = np.array(node_loss)\n","        node_neigh_class = np.array(node_neigh_class)\n","        node_heighest_loss_class = np.array(node_heighest_loss_class)\n","        sorted_index = np.argsort(node_loss)\n","        \n","        \n","        # 1. AVE without lowest loss\n","        if loss_function == 1:\n","            top_50_node = sorted_index[k-attack_num:k]\n","        # 2. Minimum loss\n","        # 3. ave(all loss) - ave( loss without lowest)\n","        else:\n","            top_50_node = sorted_index[0:attack_num]\n","        \n","        \n","        # top_50_node: node to attack\n","        # node_neigh_class: the neighbor class with the lowest loss; the class to change neighbor edge for each node\n","        \n","        edge_set = set()\n","        for [n1,n2] in self.edges:\n","            edge_set.add((n1,n2))\n","        # perform attack according to information above.\n","        for node_index in top_50_node:\n","            change_neighbor_class = node_neigh_class[node_index]\n","            my_target = self.targets[node_index]\n","            my_features = self.features[node_index]\n","            \n","            ## Another way of selecting new node target (based on highest loss)\n","            new_node_target = node_heighest_loss_class[node_index]\n","            \n","#             new_node_target = np.random.randint(self.num_classes, size=1)[0]\n","#             while new_node_target == my_target:\n","#                 new_node_target = np.random.randint(self.num_classes, size=1)[0]\n","                \n","            new_node_features = np.copy(my_features)\n","            new_node_index = len(self.targets)\n","            \n","            self.features = np.append(self.features, np.array([my_features]), axis=0)\n","            self.targets = np.append(self.targets, np.array([new_node_target]), axis=0)\n","            \n","            my_neighbors = self.neighbors[node_index]\n","            for neighbor_index in my_neighbors:\n","                if self.targets[neighbor_index] == node_neigh_class[node_index]:\n","                    edge_set.remove((node_index, neighbor_index))\n","                    edge_set.add((new_node_index, neighbor_index))\n","            edge_set.add((node_index, new_node_index))\n","            edge_set.add((new_node_index, node_index))\n","        \n","        updated_edge_list = []\n","        for (n1,n2) in edge_set:\n","            updated_edge_list.append([n1,n2])\n","        self.edges = np.array(updated_edge_list)\n","        return self.features, self.edges, self.targets"]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2022-12-12T06:39:08.976191Z","iopub.status.busy":"2022-12-12T06:39:08.975493Z","iopub.status.idle":"2022-12-12T06:39:08.986815Z","shell.execute_reply":"2022-12-12T06:39:08.985941Z","shell.execute_reply.started":"2022-12-12T06:39:08.976153Z"},"trusted":true},"outputs":[],"source":["import random\n","def set_global_determinism(seed=12345):\n","    os.environ['PYTHONHASHSEED'] = str(seed)\n","    random.seed(seed)\n","    tf.random.set_seed(seed)\n","    np.random.seed(seed)\n","\n","set_global_determinism()"]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2022-12-12T06:39:08.989148Z","iopub.status.busy":"2022-12-12T06:39:08.988497Z","iopub.status.idle":"2022-12-12T06:39:30.758716Z","shell.execute_reply":"2022-12-12T06:39:30.757614Z","shell.execute_reply.started":"2022-12-12T06:39:08.989111Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Using GCN (local pooling) filters...\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"5b7b26780dc2443782300041ad4475f1","version_major":2,"version_minor":0},"text/plain":["Training:   0%|           0/500 ETA: ?s,  ?epochs/s"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["2022-12-12 06:39:11.098199: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)\n"]},{"name":"stdout","output_type":"stream","text":["1/1 [==============================] - 0s 87ms/step - loss: 0.2076 - acc: 0.9273\n"]},{"data":{"text/plain":["0.9272525906562805"]},"execution_count":7,"metadata":{},"output_type":"execute_result"}],"source":["import stellargraph as sg\n","import tensorflow as tf\n","from tensorflow import keras\n","import json\n","import os\n","import warnings\n","import pandas as pd\n","import numpy as np\n","\n","\n","\n","import pandas as pd\n","import os\n","\n","from sklearn.decomposition import PCA\n","from sklearn.manifold import TSNE\n","\n","import stellargraph as sg\n","from stellargraph.mapper import FullBatchNodeGenerator\n","from stellargraph.layer import GCN\n","from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n","from stellargraph.layer import GAT\n","from stellargraph.mapper import DirectedGraphSAGENodeGenerator, GraphSAGENodeGenerator\n","from stellargraph.layer import DirectedGraphSAGE, GraphSAGE\n","from stellargraph import datasets\n","from tensorflow.keras import layers, optimizers, losses, metrics, Model\n","from sklearn import preprocessing, model_selection\n","from IPython.display import display, HTML\n","import matplotlib.pyplot as plt\n","%matplotlib inline\n","from tensorflow.keras import layers, optimizers, losses, metrics, Model, regularizers\n","\n","def loadData(features, edge_list, labels):\n","    node_features = pd.DataFrame(features)\n","    source = edge_list[:,0]\n","    target = edge_list[:,1]\n","    edges = pd.DataFrame({\"source\": source, \"target\": target})\n","    G = sg.StellarDiGraph(node_features, edges, node_type_default=\"paper\", edge_type_default=\"cites\")\n","    node_subjects = pd.Series(labels)\n","    return G, node_subjects\n","\n","\n","classifier_name = \"GCN\"\n","\n","\n","dataset = \"cora\"  # can also select 'pubmed'\n","\n","if dataset == \"cora\":\n","    G, node_subjects = datasets.Cora().load(directed=True)\n","elif dataset == \"pubmed\":\n","    d = datasets.PubMedDiabetes()\n","    display(HTML(d.description))\n","    G, node_subjects = d.load()\n","    \n","#     G, node_subjects = loadData(np.array(G.node_features()), np.array(G.edges), node_subjects)\n","\n","test(G, node_subjects, classifier_name)"]},{"cell_type":"code","execution_count":8,"metadata":{"execution":{"iopub.execute_input":"2022-12-12T06:39:30.761262Z","iopub.status.busy":"2022-12-12T06:39:30.760687Z","iopub.status.idle":"2022-12-12T06:39:30.769640Z","shell.execute_reply":"2022-12-12T06:39:30.768552Z","shell.execute_reply.started":"2022-12-12T06:39:30.761217Z"},"trusted":true},"outputs":[{"data":{"text/plain":["Int64Index([  31336, 1061127, 1106406,   13195,   37879, 1126012, 1107140,\n","            1102850,   31349, 1106418,\n","            ...\n","             626531, 1131180, 1130454, 1131184, 1128974, 1128975, 1128977,\n","            1128978,  117328,   24043],\n","           dtype='int64', length=2708)"]},"execution_count":8,"metadata":{},"output_type":"execute_result"}],"source":["G.nodes()"]},{"cell_type":"code","execution_count":9,"metadata":{"execution":{"iopub.execute_input":"2022-12-12T06:39:30.771301Z","iopub.status.busy":"2022-12-12T06:39:30.770918Z","iopub.status.idle":"2022-12-12T06:42:27.101608Z","shell.execute_reply":"2022-12-12T06:42:27.100450Z","shell.execute_reply.started":"2022-12-12T06:39:30.771268Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Using GCN (local pooling) filters...\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"22e14245e5c54220a74311dc55bdc87d","version_major":2,"version_minor":0},"text/plain":["Training:   0%|           0/500 ETA: ?s,  ?epochs/s"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["1/1 [==============================] - 0s 83ms/step - loss: 0.2959 - acc: 0.9125\n","Using GCN (local pooling) filters...\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"73b5778ae1294e33baa8c3ce0dcf48d4","version_major":2,"version_minor":0},"text/plain":["Training:   0%|           0/500 ETA: ?s,  ?epochs/s"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["1/1 [==============================] - 0s 82ms/step - loss: 0.2526 - acc: 0.9217\n","Using GCN (local pooling) filters...\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"364435ea5b9147b5a457a294955a176e","version_major":2,"version_minor":0},"text/plain":["Training:   0%|           0/500 ETA: ?s,  ?epochs/s"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["1/1 [==============================] - 0s 88ms/step - loss: 0.2512 - acc: 0.9228\n","Using GCN (local pooling) filters...\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"6b7b3a7a99524b41b5706e64d6fe9d5d","version_major":2,"version_minor":0},"text/plain":["Training:   0%|           0/500 ETA: ?s,  ?epochs/s"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["1/1 [==============================] - 0s 82ms/step - loss: 0.2225 - acc: 0.9269\n","Using GCN (local pooling) filters...\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"bf45668ff6ed48a9b243ce1686cb940f","version_major":2,"version_minor":0},"text/plain":["Training:   0%|           0/500 ETA: ?s,  ?epochs/s"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["1/1 [==============================] - 0s 86ms/step - loss: 0.2135 - acc: 0.9324\n","Using GCN (local pooling) filters...\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"99fabd0f941b486cba140948a37cf27b","version_major":2,"version_minor":0},"text/plain":["Training:   0%|           0/500 ETA: ?s,  ?epochs/s"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["1/1 [==============================] - 0s 91ms/step - loss: 0.2505 - acc: 0.9236\n","Using GCN (local pooling) filters...\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"194e06ec141e4435b088999fb9317972","version_major":2,"version_minor":0},"text/plain":["Training:   0%|           0/500 ETA: ?s,  ?epochs/s"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["1/1 [==============================] - 0s 87ms/step - loss: 0.2246 - acc: 0.9302\n","Using GCN (local pooling) filters...\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"2c155b9fccda4fa7996f9f9e2c90c490","version_major":2,"version_minor":0},"text/plain":["Training:   0%|           0/500 ETA: ?s,  ?epochs/s"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["1/1 [==============================] - 0s 85ms/step - loss: 0.1959 - acc: 0.9365\n","Using GCN (local pooling) filters...\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"a0263523765149c99fc60f5eb5eeb13b","version_major":2,"version_minor":0},"text/plain":["Training:   0%|           0/500 ETA: ?s,  ?epochs/s"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["1/1 [==============================] - 0s 81ms/step - loss: 0.2346 - acc: 0.9247\n","Using GCN (local pooling) filters...\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"72dc9d5068c84c108d6cf38d204eb2af","version_major":2,"version_minor":0},"text/plain":["Training:   0%|           0/500 ETA: ?s,  ?epochs/s"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["1/1 [==============================] - 0s 83ms/step - loss: 0.2608 - acc: 0.9213\n"]},{"data":{"text/plain":["0.9252584993839263"]},"execution_count":9,"metadata":{},"output_type":"execute_result"}],"source":["ata = AttackingAlgo(G.nodes(), G.node_features(), G.edges(), node_subjects)\n","attacked_features, attacked_edges, attacked_targets = ata.attack(attack_num=90, loss_function=3)\n","new_G, new_node_subjects = loadData(attacked_features, np.array(attacked_edges), attacked_targets)\n","\n","accs = []\n","for i in range(10):\n","    accs.append(test(new_G, new_node_subjects, classifier_name, attack_num=90))\n","    keras.backend.clear_session()\n","    \n","sum(accs) / 10"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"Python 3.9.7 64-bit","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.7"},"vscode":{"interpreter":{"hash":"8ae3636c20714d98f4e81b4099ee3e7bf4d05b2cc9bfad70e063a2686969b724"}}},"nbformat":4,"nbformat_minor":4}
