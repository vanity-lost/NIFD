{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import json\n",
    "import os\n",
    "import warnings\n",
    "from copy import deepcopy\n",
    "from tqdm import tqdm\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from utils import get_data\n",
    "from gcn import GNNNodeClassifier, model_fit, plot_learning_curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs, X, y, just_edges, features, labels = get_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gnn_model = GNNNodeClassifier(\n",
    "    features, \n",
    "    np.concatenate((just_edges, np.zeros((2, 100), dtype=np.int64)), axis=1), \n",
    "    num_classes=len(np.unique(labels)),\n",
    ")\n",
    "\n",
    "history = model_fit(gnn_model, X, y, verbose=0, epoches=10)\n",
    "\n",
    "plot_learning_curves(history)\n",
    "\n",
    "_, train_accuracy = gnn_model.evaluate(X, y, verbose=0)\n",
    "print(f\"Train accuracy: {round(train_accuracy * 100, 2)}%\", end='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.compat.v1.disable_eager_execution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "gnn_model.save_weights('./original')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rl.agents import DQNAgent\n",
    "from rl.policy import BoltzmannQPolicy\n",
    "from rl.memory import SequentialMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gym import Env\n",
    "from gym.spaces import Discrete, Box\n",
    "from tensorflow.keras.models import Sequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NodeEnv(py_environment.PyEnvironment):\n",
    "    def __init__(self, inputs, last_acc, original_path, state_path, classifier):\n",
    "        self._action_spec = array_spec.BoundedArraySpec(\n",
    "            shape=(), dtype=np.int32, minimum=0, maximum=len(inputs)-1, name='action'\n",
    "        )\n",
    "        self._observation_spec = array_spec.BoundedArraySpec(\n",
    "            shape=(), dtype=np.int32, name='observation'\n",
    "        )\n",
    "\n",
    "        self.classifier = classifier\n",
    "        self.classifier.save_weights(original_path)\n",
    "        self.classifier.save_weights(state_path)\n",
    "        self.original_path = original_path\n",
    "        self.state_path = state_path\n",
    "\n",
    "        self._original = inputs\n",
    "        self._episode_ended = False\n",
    "        self.original_acc = last_acc\n",
    "        \n",
    "        self.original_inputs = inputs\n",
    "\n",
    "        self._reset()\n",
    "\n",
    "    def action_spec(self):\n",
    "        return self._action_spec\n",
    "\n",
    "    def observation_spec(self):\n",
    "        return self._observation_spec\n",
    "\n",
    "    def _reset(self):\n",
    "        self._state = np.array([], dtype=np.int32)\n",
    "        self.classifier.load_weights(self.original_path)\n",
    "        self.last_acc = self.original_acc\n",
    "        self.state_inputs = deepcopy(self.original_inputs)\n",
    "        return ts.restart(self._state)\n",
    "\n",
    "    def _step(self, target_node):\n",
    "        if self._episode_ended:\n",
    "            return self.reset()\n",
    "        \n",
    "        if (target_node in self._state) or (target_node > self.state_inputs.shape[0] - 50):\n",
    "            return ts.termination(self._state, reward=-1)\n",
    "\n",
    "        self.fake_node(target_node)\n",
    "\n",
    "        if self.calc_perturb() >= 0.05 or len(self._state) >= 50:\n",
    "            return ts.termination(self._state, reward=-1)\n",
    "\n",
    "        rwd = self._get_reward()\n",
    "        return ts.transition(self._state, reward=rwd, discount=1.0)\n",
    "\n",
    "    def _get_reward(self):\n",
    "        self.classifier.load_weights(self.state_path)\n",
    "\n",
    "        history = self.classifier.fit(\n",
    "            x=np.array([i for i in range(self.state_inputs[0])]),\n",
    "            y=self.state_inputs[:, 0],\n",
    "            epochs=20,\n",
    "            batch_size=256,\n",
    "            validation_split=0.15,\n",
    "            verbose=0,\n",
    "        )\n",
    "        _, train_accuracy = self.classifier.evaluate(x=np.array([i for i in range(self.state_inputs[0])]), y=self.state_inputs[:, 0], verbose=0)\n",
    "        res = self.last_acc - train_accuracy\n",
    "\n",
    "        self.last_acc = train_accuracy\n",
    "        self.classifier.save_weights(self.state_path)\n",
    "\n",
    "        return res\n",
    "\n",
    "    def calc_perturb(self):\n",
    "        return 0.01\n",
    "\n",
    "    def fake_node(self, target_node):\n",
    "        fake_id = self.state_inputs.shape[0] - 50 + len(self._state) + 1\n",
    "        self._state = np.append(self._state, target_node).astype(np.int32)\n",
    "\n",
    "        label = np.random.randint(7)\n",
    "        while label == self.state_inputs[target_node, 0]:\n",
    "            label = np.random.randint(7)\n",
    "        self.state_inputs[fake_id, 0] = label\n",
    "\n",
    "        self.state_inputs[fake_id, 1:1434] = self.state_inputs[target_node, 1:1434]\n",
    "\n",
    "        # modify edges\n",
    "        self.state_inputs[fake_id, 1434+target_node] = 1\n",
    "        self.state_inputs[target_node, 1434+fake_id] = 1\n",
    "\n",
    "        for i in range(len(self.state_inputs)):\n",
    "            if self.state_inputs[i, target_node+1434] == 1 and np.random.random() < 0.5:\n",
    "                self.state_inputs[i, fake_id+1434] = 1\n",
    "            elif self.state_inputs[target_node, i+1434] == 1 and np.random.random() < 0.5:\n",
    "                self.state_inputs[fake_id, i+1434] = 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gym import Env\n",
    "from gym.spaces import Discrete, Box\n",
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "\n",
    "states = env.observation_space.shape\n",
    "actions = env.action_space.n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(states, actions):\n",
    "    model = Sequential()    \n",
    "    model.add(Dense(24, activation='relu', input_shape=states))\n",
    "    model.add(Dense(24, activation='relu'))\n",
    "    model.add(Dense(actions, activation='linear'))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_model(states, actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rl.agents import DQNAgent\n",
    "from rl.policy import BoltzmannQPolicy\n",
    "from rl.memory import SequentialMemory\n",
    "\n",
    "from gym import Env\n",
    "from gym.spaces import Discrete, Box\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ShowerEnv(Env):\n",
    "    def __init__(self, inputs, last_acc, original_path, state_path, classifier):\n",
    "        self.action_space = Discrete(len(inputs))\n",
    "        self.observation_space = Box(low=np.array([0]), high=np.array([50]))\n",
    "        \n",
    "        self.classifier = classifier\n",
    "        self.classifier.save_weights(original_path)\n",
    "        self.classifier.save_weights(state_path)\n",
    "        self.original_path = original_path\n",
    "        self.state_path = state_path\n",
    "\n",
    "        self.original_acc = last_acc\n",
    "        self.original_inputs = inputs\n",
    "\n",
    "        self.state = self.reset()\n",
    "        \n",
    "    def step(self, target_node):\n",
    "        if (target_node in self.attacked_node) or (target_node > self.state_inputs.shape[0] - 50):\n",
    "            done = True\n",
    "\n",
    "        self.fake_node(target_node)\n",
    "\n",
    "        done = False\n",
    "        if self.calc_perturb() >= 0.05 or self.state >= 50:\n",
    "            done = True\n",
    "\n",
    "        rwd = self._get_reward()\n",
    "        \n",
    "        self.state += 1\n",
    "        info = {}\n",
    "        return self.state, reward, done, info\n",
    "    \n",
    "    def _get_reward(self):\n",
    "        self.classifier.load_weights(self.state_path)\n",
    "\n",
    "        history = self.classifier.fit(\n",
    "            x=np.array([i for i in range(self.state_inputs[0])]),\n",
    "            y=self.state_inputs[:, 0],\n",
    "            epochs=20,\n",
    "            batch_size=256,\n",
    "            validation_split=0.15,\n",
    "            verbose=0,\n",
    "        )\n",
    "        _, train_accuracy = self.classifier.evaluate(x=np.array([i for i in range(self.state_inputs[0])]), \n",
    "                                                     y=self.state_inputs[:, 0], verbose=0)\n",
    "        res = self.last_acc - train_accuracy\n",
    "\n",
    "        self.last_acc = train_accuracy\n",
    "        self.classifier.save_weights(self.state_path)\n",
    "\n",
    "        return res\n",
    "\n",
    "\n",
    "    def render(self):\n",
    "        # Implement viz\n",
    "        pass\n",
    "    \n",
    "    def reset(self):\n",
    "        self.state = 0\n",
    "        self.classifier.load_weights(self.original_path)\n",
    "        self.last_acc = self.original_acc\n",
    "        self.state_inputs = deepcopy(self.original_inputs)   \n",
    "        self.attacked_node = set()\n",
    "        return self.state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(states, actions):\n",
    "    model = Sequential()    \n",
    "    model.add(Dense(24, activation='relu', input_shape=states))\n",
    "    model.add(Dense(24, activation='relu'))\n",
    "    model.add(Dense(actions, activation='linear'))\n",
    "    return model\n",
    "\n",
    "def build_agent(model, actions):\n",
    "    policy = BoltzmannQPolicy()\n",
    "    memory = SequentialMemory(limit=50000, window_length=1)\n",
    "    dqn = DQNAgent(model=model, memory=memory, policy=policy, \n",
    "                  nb_actions=actions, nb_steps_warmup=10, target_model_update=1e-2)\n",
    "    return dqn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, train_accuracy = gnn_model.evaluate(X, y, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = ShowerEnv(inputs, 1, './original', './state', gnn_model)\n",
    "\n",
    "states = env.observation_space.shape\n",
    "actions = env.action_space.n\n",
    "model = build_model(states, actions)\n",
    "dqn = build_agent(model, actions)\n",
    "dqn.compile(keras.optimizers.Adam(lr=1e-3), metrics=['mae'])\n",
    "dqn.fit(env, nb_steps=50000, visualize=False, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "environment = NodeEnv(inputs.astype(np.int32), 1, './original', './state', gnn_model)\n",
    "tf_env = tf_py_environment.TFPyEnvironment(environment)\n",
    "\n",
    "from tf_agents.specs import tensor_spec\n",
    "from tf_agents.networks import sequential\n",
    "from tf_agents.drivers import py_driver\n",
    "from tf_agents.agents.dqn import dqn_agent\n",
    "from tf_agents.utils import common\n",
    "\n",
    "fc_layer_params = (100, 50)\n",
    "action_tensor_spec = tensor_spec.from_spec(tf_env.action_spec())\n",
    "num_actions = action_tensor_spec.maximum - action_tensor_spec.minimum + 1\n",
    "\n",
    "def dense_layer(num_units):\n",
    "    return tf.keras.layers.Dense(\n",
    "      num_units,\n",
    "      activation=tf.keras.activations.relu,\n",
    "      kernel_initializer=tf.keras.initializers.VarianceScaling(\n",
    "          scale=2.0, mode='fan_in', distribution='truncated_normal')\n",
    "    )\n",
    "\n",
    "dense_layers = [dense_layer(num_units) for num_units in fc_layer_params]\n",
    "q_values_layer = tf.keras.layers.Dense(\n",
    "    num_actions,\n",
    "    activation=None,\n",
    "    kernel_initializer=tf.keras.initializers.RandomUniform(minval=-0.03, maxval=0.03),\n",
    "    bias_initializer=tf.keras.initializers.Constant(-0.2))\n",
    "q_net = sequential.Sequential(dense_layers + [q_values_layer])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam(learning_rate=1e-3)\n",
    "train_step_counter = tf.Variable(0)\n",
    "\n",
    "agent = dqn_agent.DqnAgent(\n",
    "    tf_env.time_step_spec(),\n",
    "    tf_env.action_spec(),\n",
    "    q_network=q_net,\n",
    "    optimizer=optimizer,\n",
    "    td_errors_loss_fn=common.element_wise_squared_loss,\n",
    "    train_step_counter=train_step_counter\n",
    ")\n",
    "\n",
    "agent.initialize()\n",
    "eval_policy = agent.policy\n",
    "collect_policy = agent.collect_policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_avg_return(environment, policy, num_episodes=10):\n",
    "    total_return = 0.0\n",
    "    for _ in range(num_episodes):\n",
    "\n",
    "        time_step = environment.reset()\n",
    "        episode_return = 0.0\n",
    "\n",
    "        while not time_step.is_last():\n",
    "            action_step = policy.action(time_step)\n",
    "            time_step = environment.step(action_step.action)\n",
    "            episode_return += time_step.reward\n",
    "        total_return += episode_return\n",
    "\n",
    "    avg_return = total_return / num_episodes\n",
    "    return avg_return.numpy()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tf_agents.policies import random_tf_policy\n",
    "random_policy = random_tf_policy.RandomTFPolicy(tf_env.time_step_spec(),\n",
    "                                                tf_env.action_spec())\n",
    "time_step = tf_env.reset()\n",
    "random_policy.action(time_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (Optional) Optimize by wrapping some of the code in a graph using TF function.\n",
    "agent.train = common.function(agent.train)\n",
    "\n",
    "# Reset the train step.\n",
    "agent.train_step_counter.assign(0)\n",
    "\n",
    "# Evaluate the agent's policy once before training.\n",
    "avg_return = compute_avg_return(tf_env, agent.policy, 10)\n",
    "returns = [avg_return]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "8ae3636c20714d98f4e81b4099ee3e7bf4d05b2cc9bfad70e063a2686969b724"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
